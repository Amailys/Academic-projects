{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56badda",
   "metadata": {},
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbef4d",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook we code one version of the [Proximal Policy Optimization\n",
    "(PPO)](https://arxiv.org/pdf/1707.06347.pdf) algorithms using BBRL. More\n",
    "precisely, the version here is the one that clips the policy gradient.\n",
    "\n",
    "The PPO algorithm is superficially explained in [this\n",
    "video](https://www.youtube.com/watch?v=uRNL93jV2HE) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/10_ppo.pdf).\n",
    "\n",
    "It is also a good idea to have a look at the [spinning up\n",
    "documentation](https://spinningup.openai.com/en/latest/algorithms/ppo.html).\n",
    "\n",
    "This version of PPO works, but it incorrectly samples minibatches randomly\n",
    "from the rollouts without making sure that each sample is used once and only\n",
    "once See:\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/ for a\n",
    "full description of all the coding tricks that should be integrated\n",
    "\n",
    "# Setting up the environment\n",
    "We first need to setup the environment\n",
    "Installs the necessary Python and system libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f70e2a",
   "metadata": {},
   "source": [
    "# Setting up the environment\n",
    "We first need to setup the environment\n",
    "Installs the necessary Python and system libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4025a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amailys/deepdac/lib/python3.12/site-packages/bbrl_utils/notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n",
      "error: XDG_RUNTIME_DIR is invalid or not set in the environment.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "    assert run([\"pip\", \"install\", \"easypip\"]).returncode == 0, \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup()\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, KWAgentWrapper, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpisodicAlgo, iter_partial_episodes\n",
    "from bbrl_utils.nn import build_ortho_mlp, setup_optimizer\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from bbrl_utils.nn import copy_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050d057",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "        # Number of steps (partial iteration)\n",
    "        \"n_steps\": 100,\n",
    "        \n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "\n",
    "\n",
    "Besides, it also defines an `iter_episodes` that allows to iterate over partial\n",
    "episodes (with `n_steps` from `n_envs` environments):\n",
    "\n",
    "```python3\n",
    "  # with partial episodes\n",
    "  for workspace in algo.iter_partial_episodes():\n",
    "      # workspace is a workspace containing 50 transitions\n",
    "      # (with autoreset)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d38f9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Definition of PPO agents\n",
    "\n",
    "## Critic agent\n",
    "\n",
    "As A2C, PPO uses a value function $V(s)$. We thus call upon the `VAgent`\n",
    "class,  which takes an observation as input and whose output is the value of\n",
    "this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36e59c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class VAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, name=\"critic\"):\n",
    "        super().__init__(name)\n",
    "        self.is_q_function = False\n",
    "        self.model = build_ortho_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        critic = self.model(observation).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}v_values\", t), critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e7827",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The DiscretePolicy\n",
    "\n",
    "The DiscretePolicy was already used in A2C to deal with discrete actions, but\n",
    "we have added the possibility to only predict the probability of an action\n",
    "using the ```predict_proba``` variable in the ```forward()``` function. The\n",
    "code is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807c0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePolicy(Agent):\n",
    "    def __init__(self, state_dim, hidden_size, n_actions, name=\"policy\"):\n",
    "        super().__init__(name=name)\n",
    "        self.model = build_ortho_mlp(\n",
    "            [state_dim] + list(hidden_size) + [n_actions], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def dist(self, obs):\n",
    "        scores = self.model(obs)\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "        return torch.distributions.Categorical(probs)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        t,\n",
    "        *,\n",
    "        stochastic=True,\n",
    "        predict_proba=False,\n",
    "        compute_entropy=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the action given either a time step (looking into the workspace)\n",
    "        or an observation (in kwargs)\n",
    "        \"\"\"\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        scores = self.model(observation)\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        if predict_proba:\n",
    "            action = self.get((\"action\", t))\n",
    "            log_probs = probs[torch.arange(probs.size()[0]), action].log()\n",
    "            self.set((f\"{self.prefix}logprob_predict\", t), log_probs)\n",
    "        else:\n",
    "            if stochastic:\n",
    "                action = torch.distributions.Categorical(probs).sample()\n",
    "            else:\n",
    "                action = scores.argmax(1)\n",
    "            self.set((\"action\", t), action)\n",
    "\n",
    "        if compute_entropy:\n",
    "            entropy = torch.distributions.Categorical(probs).entropy()\n",
    "            self.set((f\"{self.prefix}entropy\", t), entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b988e1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Main PPO agent\n",
    "\n",
    "In the following, we create the PPO Agent, with one policy and one critic,\n",
    "and their \"delayed\" versions (target network for the critic, and previous \n",
    "policy in the inner loop of the optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b2213d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PPOClip(EpisodicAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg, autoreset=True)\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        self.train_policy = globals()[cfg.algorithm.policy_type](\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.actor_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"current_policy/\")\n",
    "\n",
    "        self.eval_policy = KWAgentWrapper(\n",
    "            self.train_policy, \n",
    "            stochastic=False,\n",
    "            predict_proba=False,\n",
    "            compute_entropy=False,\n",
    "        )\n",
    "\n",
    "        self.critic_agent = VAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size\n",
    "        ).with_prefix(\"critic/\")\n",
    "        self.old_critic_agent = copy.deepcopy(self.critic_agent).with_prefix(\"old_critic/\")\n",
    "\n",
    "        self.old_policy = copy.deepcopy(self.train_policy)\n",
    "        self.old_policy.with_prefix(\"old_policy/\")\n",
    "\n",
    "        self.policy_optimizer = setup_optimizer(\n",
    "            cfg.optimizer, self.train_policy\n",
    "        )\n",
    "        self.critic_optimizer = setup_optimizer(\n",
    "            cfg.optimizer, self.critic_agent\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be8698",
   "metadata": {},
   "source": [
    "In the cell below, we optimize the policy loss for PPO-clip, i.e.\n",
    "\n",
    "$$\n",
    "L^{C L I P}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\operatorname{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right]\n",
    "$$\n",
    "where $$r_t(\\theta) = \\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)}$$\n",
    "\n",
    "Useful torch functions:\n",
    "- [torch.clamp](https://pytorch.org/docs/stable/generated/torch.clamp.html) computes $\\min(\\max(x_i, m_i), M_i)$ where $m_i$ and $M_i$ are the lower and upper bounds respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9074ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbrl.utils.functional import gae\n",
    "def run(ppo_clip: PPOClip):\n",
    "    cfg = ppo_clip.cfg\n",
    "\n",
    "    t_policy = TemporalAgent(ppo_clip.train_policy)\n",
    "    t_old_policy = TemporalAgent(ppo_clip.old_policy)\n",
    "    t_critic = TemporalAgent(ppo_clip.critic_agent)\n",
    "    t_old_critic = TemporalAgent(ppo_clip.old_critic_agent)\n",
    "\n",
    "    for train_workspace in iter_partial_episodes(\n",
    "        ppo_clip, cfg.algorithm.n_steps\n",
    "    ):\n",
    "        # Run the current policy and evaluate the proba of its action according\n",
    "        # to the old policy The old_policy can be run after the train_agent on\n",
    "        # the same workspace because it writes a logprob_predict and not an\n",
    "        # action. That is, it does not determine the action of the old_policy,\n",
    "        # it just determines the proba of the action of the current policy given\n",
    "        # its own probabilities\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_old_policy(\n",
    "                train_workspace,\n",
    "                t=0,\n",
    "                n_steps=cfg.algorithm.n_steps,\n",
    "                # Just computes the probability of the old policy's action\n",
    "                # to get the ratio of probabilities\n",
    "                predict_proba=True,\n",
    "                compute_entropy=False,\n",
    "            )\n",
    "\n",
    "        # Compute the critic value over the whole workspace\n",
    "        t_critic(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n",
    "        with torch.no_grad():\n",
    "            t_old_critic(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n",
    "\n",
    "        ws_terminated, ws_reward, ws_v_value, ws_old_v_value = train_workspace[\n",
    "            \"env/terminated\",\n",
    "            \"env/reward\",\n",
    "            \"critic/v_values\",\n",
    "            \"old_critic/v_values\",\n",
    "        ]\n",
    "\n",
    "        # the critic values are clamped to move not too far away from the values of the previous critic\n",
    "        if cfg.algorithm.clip_range_vf > 0:\n",
    "            # Clip the difference between old and new values\n",
    "            # NOTE: this depends on the reward scaling\n",
    "            ws_v_value = ws_old_v_value + torch.clamp(\n",
    "                ws_v_value - ws_old_v_value,\n",
    "                -cfg.algorithm.clip_range_vf,\n",
    "                cfg.algorithm.clip_range_vf,\n",
    "            )\n",
    "\n",
    "        # Compute the advantage using the (clamped) critic values\n",
    "        with torch.no_grad():\n",
    "            advantage = gae(\n",
    "                ws_reward[1:],\n",
    "                ws_v_value[1:],\n",
    "                ~ws_terminated[1:],\n",
    "                ws_v_value[:-1],\n",
    "                cfg.algorithm.discount_factor,\n",
    "                cfg.algorithm.gae,\n",
    "            )\n",
    "\n",
    "        ppo_clip.critic_optimizer.zero_grad()\n",
    "        target = ws_reward[1:] + cfg.algorithm.discount_factor * ws_old_v_value[1:].detach() * (1 - ws_terminated[1:].int())\n",
    "        critic_loss = torch.nn.functional.mse_loss(ws_v_value[:-1], target) * cfg.algorithm.critic_coef\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            ppo_clip.critic_agent.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        ppo_clip.critic_optimizer.step()\n",
    "\n",
    "        # We store the advantage into the transition_workspace\n",
    "        if cfg.algorithm.normalize_advantage and advantage.shape[1] > 1:\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "        train_workspace.set_full(\"advantage\", torch.cat(\n",
    "            (advantage, torch.zeros(1, advantage.shape[1]))\n",
    "        ))\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "\n",
    "        # Inner optimization loop: we sample transitions and use them to learn\n",
    "        # the policy\n",
    "        for opt_epoch in range(cfg.algorithm.opt_epochs):\n",
    "            if cfg.algorithm.batch_size > 0:\n",
    "                sample_workspace = transition_workspace.select_batch_n(\n",
    "                    cfg.algorithm.batch_size\n",
    "                )\n",
    "            else:\n",
    "                sample_workspace = transition_workspace\n",
    "\n",
    "            # Compute the policy loss\n",
    "\n",
    "            policy_advantage = sample_workspace['advantage'][0]\n",
    "\n",
    "\n",
    "            # Compute the probability of the played actions according to the current policy\n",
    "            # We do not replay the action: we use the one stored into the dataset\n",
    "            # Note that the policy is not wrapped into a TemporalAgent, but we use a single step\n",
    "            #Compute the ratio of action probabilities\n",
    "            # Compute the policy loss\n",
    "\n",
    "            \n",
    "            ppo_clip.train_policy(sample_workspace,t=0,predict_proba=True,compute_entropy=True)\n",
    "            \n",
    "            old_action_probs = torch.exp(sample_workspace['old_policy/logprob_predict'][0])\n",
    "            current_action_probs = torch.exp(sample_workspace['current_policy/logprob_predict'][0])\n",
    "\n",
    "            rt = current_action_probs/old_action_probs\n",
    "            clipped_ratio = torch.clamp(rt, 1 - cfg.algorithm.clip_range, 1 + cfg.algorithm.clip_range)\n",
    "            policy_loss = torch.mean(torch.min( rt* policy_advantage, clipped_ratio*policy_advantage ))\n",
    "\n",
    "\n",
    "            loss_policy = -cfg.algorithm.policy_coef * policy_loss\n",
    "\n",
    "\n",
    "            entropy = sample_workspace[\"current_policy/entropy\"]\n",
    "            assert len(entropy) == 1, f\"{entropy.shape}\"\n",
    "            entropy_loss = entropy[0].mean()\n",
    "            loss_entropy = -cfg.algorithm.entropy_coef * entropy_loss\n",
    "\n",
    "            # Store the losses for tensorboard display\n",
    "            ppo_clip.logger.log_losses(\n",
    "                critic_loss, entropy_loss, policy_loss, ppo_clip.nb_steps\n",
    "            )\n",
    "            ppo_clip.logger.add_log(\n",
    "                \"advantage\", policy_advantage[0].mean(), ppo_clip.nb_steps\n",
    "            )\n",
    "\n",
    "            loss = loss_policy + loss_entropy\n",
    "\n",
    "            ppo_clip.policy_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                ppo_clip.train_policy.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            ppo_clip.policy_optimizer.step()\n",
    "\n",
    "        # Copy parameters\n",
    "        copy_parameters(ppo_clip.train_policy, ppo_clip.old_policy)\n",
    "        copy_parameters(ppo_clip.critic_agent, ppo_clip.old_critic_agent)\n",
    "\n",
    "        # Evaluates our current algorithm if needed\n",
    "        ppo_clip.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3294d8c",
   "metadata": {},
   "source": [
    "# Definition of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608c8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_dir\": \"${gym_env.env_name}/ppo-clip-S${algorithm.seed}_${current_time:}\",\n",
    "    \"save_best\": False,\n",
    "    \"logger\": {\n",
    "        \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "        \"cache_size\": 10000,\n",
    "        \"every_n_seconds\": 10,\n",
    "        \"verbose\": False,\n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 12,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"eval_interval\": 1000,\n",
    "        \"nb_evals\": 10,\n",
    "        \"gae\": 0.8,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"normalize_advantage\": False,\n",
    "        \"max_epochs\": 5_000,\n",
    "        \"opt_epochs\": 10,\n",
    "        \"batch_size\": 256,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"clip_range_vf\": 0,\n",
    "        \"entropy_coef\": 2e-7,\n",
    "        \"policy_coef\": 1,\n",
    "        \"critic_coef\": 1.0,\n",
    "        \"policy_type\": \"DiscretePolicy\",\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [64, 64],\n",
    "            \"critic_hidden_size\": [64, 64],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.AdamW\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"eps\": 1e-5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29631f73",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc32c4a",
   "metadata": {
    "title": "For Colab - otherwise, it is easier and better to launch tensorboard from"
   },
   "outputs": [],
   "source": [
    "# the terminal\n",
    "\n",
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71af5853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac6bae4693f4a2fad867390f2230809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ppo_clip \u001b[38;5;241m=\u001b[39m PPOClip(OmegaConf\u001b[38;5;241m.\u001b[39mcreate(params))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppo_clip\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 143\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(ppo_clip)\u001b[0m\n\u001b[1;32m    140\u001b[0m copy_parameters(ppo_clip\u001b[38;5;241m.\u001b[39mcritic_agent, ppo_clip\u001b[38;5;241m.\u001b[39mold_critic_agent)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Evaluates our current algorithm if needed\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m \u001b[43mppo_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.12/site-packages/bbrl_utils/algorithms.py:157\u001b[0m, in \u001b[0;36mRLBase.evaluate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_eval_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_steps\n\u001b[1;32m    156\u001b[0m eval_workspace \u001b[38;5;241m=\u001b[39m Workspace()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_workspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_variable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv/done\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m rewards \u001b[38;5;241m=\u001b[39m eval_workspace[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/cumulated_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_evaluation(rewards)\n",
      "File \u001b[0;32m~/deepdac/lib/python3.12/site-packages/bbrl/agents/utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m _t \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         s \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[0;32m~/deepdac/lib/python3.12/site-packages/bbrl/agents/utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m---> 31\u001b[0m         \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.12/site-packages/bbrl/agents/agent.py:84\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m workspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agent.__call__] workspace must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m workspace\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.12/site-packages/bbrl/agents/gymnasium.py:480\u001b[0m, in \u001b[0;36mParallelGymAgent.forward\u001b[0;34m(self, t, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(k, dict_slice(k, action))\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# Use last frame\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_frame[k]\n",
      "File \u001b[0;32m~/deepdac/lib/python3.12/site-packages/bbrl/agents/gymnasium.py:433\u001b[0m, in \u001b[0;36mParallelGymAgent._step\u001b[0;34m(self, k, action)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timestep[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulated_reward[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_obs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deepdac/lib/python3.12/site-packages/bbrl/agents/gymnasium.py:361\u001b[0m, in \u001b[0;36mParallelGymAgent._format_obs\u001b[0;34m(self, k, obs, info, terminated, truncated, reward)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_obs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: value\n\u001b[1;32m    354\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m ParallelGymAgent\u001b[38;5;241m.\u001b[39m_flatten_value(observation)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    355\u001b[0m         }\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation must be a torch.Tensor or a dict, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(observation)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m     )\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_obs\u001b[39m(\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m, k: \u001b[38;5;28mint\u001b[39m, obs, info, \u001b[38;5;241m*\u001b[39m, terminated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, truncated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    363\u001b[0m ):\n\u001b[1;32m    364\u001b[0m     observation: Union[Tensor, Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m ParallelGymAgent\u001b[38;5;241m.\u001b[39m_format_frame(\n\u001b[1;32m    365\u001b[0m         obs\n\u001b[1;32m    366\u001b[0m     )\n\u001b[1;32m    368\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo_clip = PPOClip(OmegaConf.create(params))\n",
    "run(ppo_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_clip.visualize_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f166a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python (deepdac)",
   "language": "python",
   "name": "deepdac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
